{
  "name": "Cudachessbot-website",
  "tagline": "",
  "body": "\r\n# CudaChessBot\r\n\r\n\r\nKevin Lee and Manik Panwar\r\n\r\n\r\nSummary\r\n\r\n\r\nWe are going to implement a parallelized chess AI using CUDA. The goal of the project will be\r\nto create an AI that can compete as well as possible in timed chess matches.\r\n\r\n\r\nBackground\r\n\r\n\r\nCurrently, the accepted best algorithm to write a chess AI is minimax augmented with\r\nalpha-beta pruning. This algorithm creates a tree of possible moves, and alpha-beta pruning is\r\nsimply used to ensure that no time is wasted in exploring worse moves than have already been\r\nseen.\r\n\r\nHowever, the tree of possible moves from a given state in chess is way too large for a computer\r\nto completely examine, so we will have to stop the algorithm at some point and judge the state\r\nusing various heuristics. We will take advantage of parallelism when we traverse the game tree\r\nin order to reduce the amount of time spent calculating each move since we want to be able to\r\nuse our chess AI to compete in timed matches.\r\n\r\nChallenge\r\n\r\n\r\nThe main challenge in this assignment will be parallelizing the minimax augmented with\r\nalpha-beta pruning algorithm. We have already used this algorithm in 15-210, but we used it in a\r\nsequential manner because alpha-beta pruning is an inherently sequential algorithm. As such,\r\nwe will have to make some sacrifices when we implement it in parallel.\r\nIn order for alpha-beta pruning to still work, we have to ensure that the communication costs are\r\nkept at a minimum so that each CUDA thread can have the current progress in the traversal of\r\nthe game tree without bogging down our computation time with communication costs. Since the\r\nwork of each thread is dependant on the work of all the other threads, minimizing\r\ncommunication will probably be the main issue we face.\r\n\r\n\r\nResources\r\n\r\n\r\nWe will be using the GHC machines running CUDA and running our code on the NVIDIA GTX\r\n1080 GPUs.\r\n\r\n\r\nPlatform Choice\r\n\r\n\r\nWe are using GPU’s because we would like to be able to simulate each of our possible moves\r\nin chess at any state on different cores each running parallely. Each of these possible moves\r\nwill in turn spawn off new searches on separate cores trying to predict what the opponent does\r\nso having lot’s of cores helps us. We are also curious how memory performance will be when\r\nrunning this on a GPU and what sort of tradeoffs we will have to see. The project will be done in\r\nC++.\r\n\r\n\r\nGoals and Deliverables\r\n\r\n\r\n1. Minimax augmented alpha beta pruning AI running parallely on NVIDIA GTX 1080\r\n2. Results of performance of AI (win ratio) as compared to number of\r\nprocessors/communication frequency between threads.\r\n3. Analysis on memory throughput of the program and cache misses of the program and\r\nhow it affects performance.\r\n4. Suggested improvements on the minimax augmented alpha beta pruning algorithm\r\nspecifically for this architecture and problem along with the tradeoffs between all the\r\niterations that were tried.\r\n\r\n\r\nSchedule:\r\n\r\n\r\n1. Week 1: Come up with outline of the solution and an initial approach to parallelism\r\n2. Week 2: Have the sequential implementation working\r\n3. Week 3: Parallelise sequential implementation to come up with parallel implementation\r\nv1.\r\nMidpoint\r\n4. Week 4: Do profiling tests on the implementation to come up with potential areas of\r\nimprovement. Iterate.\r\n5. Week 5: Iterate on implementation to better performance. Stretch Goal: Implement the\r\nsolution on CPU and see how the performance is affected as compared to GPU w/\r\nCUDA.\r\n6. Week 6: Prepare final project writeup and presentation.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}